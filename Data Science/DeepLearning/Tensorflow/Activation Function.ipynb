{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6023ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation function\n",
    "\n",
    "\n",
    "# Activation functions are used in artificial neural networks to introduce non-linearity in the output of each neuron. \n",
    "# In simple terms, activation functions determine the output of a neuron given an input or set of inputs. \n",
    "# Without activation functions, the output of each neuron would be a simple linear function of its inputs,\n",
    "# making it difficult to model complex relationships between inputs and outputs in a neural network. \n",
    "# By introducing non-linearity through activation functions, neural networks are able to model complex, \n",
    "# non-linear relationships between inputs and outputs, making them more powerful and versatile for a wide range of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68dee848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bccc95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edbfa7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3da83be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.928749847963918e-22"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f44d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this means the range of sigmoid function is from 0 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bebc2ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6681877721681662"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aac7e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"math.exp\" in the code is a mathematical function that returns the exponential value of a given input number (x).\n",
    "# The exponential value of x is defined as the result of raising the constant \"e\" (approximately equal to 2.718) to \n",
    "# the power of x. So, the function \"tanh(x)\" computes the hyperbolic tangent of x using the exponential function.\n",
    "\n",
    "def tanh(x):\n",
    "    return (math.exp(x) - math.exp(-x)) / (math.exp(x) + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d26ed89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(-56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e24d09dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1b86215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7615941559557649"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3db77169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The relu (rectified linear unit) function is a commonly used activation function in neural network models. \n",
    "# It returns the input x if x is positive, and returns 0 if x is negative. This function introduces non-linearity in \n",
    "# the model, which allows the model to learn complex representations of the input data.\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return max(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fdbe4e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c51b973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03b68e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The leaky_relu function is a type of activation function used in neural networks. \n",
    "# It is similar to the traditional ReLU activation function but with a slight difference -\n",
    "# instead of setting the value of a negative input to zero, the leaky_relu function sets it to a small negative value\n",
    "# (0.1 times the input, in this case). This helps in avoiding the \"dying ReLU\" problem, \n",
    "# where a large number of neurons can become \"dead\" during training and stop providing any meaningful contribution \n",
    "# to the model's predictions.\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return max(0.1*x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68ed1c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_relu(-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d77c0270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_relu(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5306a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why we used relu and leaky_relu function here\n",
    "\n",
    "# The activation functions in neural networks are used to introduce non-linearity into the output of each layer. \n",
    "# They transform the input signal into an output signal and the activation function decides whether a neuron should \n",
    "# be activated or not. ReLU and leaky ReLU are commonly used activation functions in neural networks as they have some \n",
    "# desirable mathematical properties and have been found to work well in practice for many tasks.\n",
    "\n",
    "# ReLU (rectified linear unit) function sets all negative values to 0 and all positive values unchanged, resulting in \n",
    "# sparse activations and reducing the computational cost. However, the standard ReLU function has a drawback of the\n",
    "# \"dying ReLU problem,\" where a neuron can get stuck in the \"dead\" state with output 0 if it experiences negative inputs.\n",
    "# To address this, the leaky ReLU function allows small negative values to \"leak\" through, rather than being set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb424a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0801b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b7c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9461ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa71f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234737ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019f12f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b38972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf1fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d823420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8679c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3829e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b0fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3ca39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d3b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff2c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7cf24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c8afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2079b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d4d78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9269604b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dfc264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793c1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b912d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
